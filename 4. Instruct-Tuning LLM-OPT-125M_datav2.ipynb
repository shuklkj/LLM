{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8637efd2-9670-4ee4-9801-8140b433e98c",
   "metadata": {},
   "source": [
    "# Final Project\n",
    "## ADSP 32021 IP01 Machine Learning Operations\n",
    "### 4. Instruct-Tuning LLM\n",
    "#### Group 2: Maria Clarissa Fionalita, Kajal Shukla, Mia Zhang, Priya Suvvaru Venkata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c4f6cd6d-2767-4ac6-bbbf-8e770d339726",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.10.13\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5dbd6084-986d-4c8f-822e-9e6a396fbdf1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc5571be-3eb2-4226-ba0b-bb7d9c458c79",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from transformers import logging\n",
    "\n",
    "# logging.set_verbosity_warning()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aae855ad-d586-4e7a-a6ac-44ebdfe080c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from huggingface_hub import notebook_login\n",
    "# # https://huggingface.co/settings/tokens\n",
    "\n",
    "# notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b7e8ed-b84e-4c6a-bb51-03bd8d4ef8b0",
   "metadata": {},
   "source": [
    "# Load Training Data\n",
    "\n",
    "https://huggingface.co/docs/datasets/loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6e7a26f4-0b63-4c4f-9b5b-feefb94be355",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "\n",
    "import datasets\n",
    "from datasets import load_dataset\n",
    "\n",
    "from transformers import DefaultDataCollator, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2452cf51-88dc-4215-9111-50b3e89f9162",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_files = {\n",
    "              \"train\": \"gs://capstone-team-green/mlops_data/finetune_data/training_data_v2.json\",\n",
    "              \"validation\": \"gs://capstone-team-green/mlops_data/finetune_data/validation_data_v2.json\",\n",
    "              \"test\": \"gs://capstone-team-green/mlops_data/finetune_data/test_data_v2.json\"\n",
    "    }\n",
    "\n",
    "data = load_dataset(\"json\", data_files = data_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "efa4551e-d06c-4cf2-83f9-2b41b98a3410",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['context', 'question', 'is_impossible', 'answers_text', 'url', 'id', 'answers'],\n",
       "        num_rows: 47717\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['context', 'question', 'is_impossible', 'answers_text', 'url', 'id', 'answers'],\n",
       "        num_rows: 6273\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['context', 'question', 'is_impossible', 'answers_text', 'url', 'id', 'answers'],\n",
       "        num_rows: 6107\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e06bf9a1-ac8f-4515-ab52-4551820caaf2",
   "metadata": {},
   "source": [
    "# Training Data Pre-Processing\n",
    "\n",
    "https://huggingface.co/docs/transformers/tasks/question_answering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a33f36f-7ade-4b12-b229-7fce2a6c23b6",
   "metadata": {},
   "source": [
    "## Load Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "15892640-3162-4790-ae2b-21516a878839",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, True)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = \"facebook/opt-125m\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "tokenizer.add_eos_token = True\n",
    "tokenizer.add_bos_token, tokenizer.add_eos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cf5fdb4-5464-48e7-9445-390aa32c3c7e",
   "metadata": {},
   "source": [
    "## Create a preprocess_function to \n",
    "Tokenize the input text and label\n",
    "\n",
    "[Alpaca LoRA's finetuning method](https://github.com/tloen/alpaca-lora/blob/main/finetune.py)nsors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8003d09b-0691-43b2-bbb7-ebaa8db94026",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cutoff_len: int = 256\n",
    "\n",
    "def tokenize(prompt, add_eos_token=True):\n",
    "    # there's probably a way to do this with the tokenizer settings\n",
    "    # but again, gotta move fast\n",
    "    result = tokenizer(\n",
    "        prompt,\n",
    "        truncation = True,\n",
    "        max_length = cutoff_len,\n",
    "        padding = \"max_length\",\n",
    "        return_tensors=None,\n",
    "    )\n",
    "    if (\n",
    "        result[\"input_ids\"][-1] != tokenizer.eos_token_id\n",
    "        and len(result[\"input_ids\"]) < cutoff_len\n",
    "        and add_eos_token\n",
    "    ):\n",
    "        result[\"input_ids\"].append(tokenizer.eos_token_id)\n",
    "        result[\"attention_mask\"].append(1)\n",
    "\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "\n",
    "    return result\n",
    "\n",
    "def format_prompt(sample):\n",
    "    prompt = f\"\"\"### Instruction: You are a helpful assistant that can answer medical questions. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know.\n",
    "\n",
    "    ### Context information is below:\n",
    "    {sample['context']}\n",
    "\n",
    "    ### Given the context information and not prior knowledge, answer the question: {sample['question']}\n",
    "    \n",
    "    ### answer: {sample[\"answers_text\"]}\n",
    "    \"\"\"\n",
    "    return prompt\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    full_prompt = format_prompt(examples)\n",
    "\n",
    "    tokenized_full_prompt = tokenize(full_prompt)\n",
    "    \n",
    "    return tokenized_full_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2d9ffe5d-3fd4-441c-9560-4fb46630b426",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8af415aacf04895823b23f852e803a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6273 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 35.1 s, sys: 4.37 s, total: 39.5 s\n",
      "Wall time: 27.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "train_data = data[\"train\"].map(preprocess_function, remove_columns=list(data[\"train\"].features))\n",
    "validation_data = data[\"validation\"].map(preprocess_function, remove_columns=list(data[\"validation\"].features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d855b3ec-3fc9-48b5-99e1-641df98e6d4d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create data collator\n",
    "data_collator = DefaultDataCollator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5127e5eb-51f5-4623-ba9d-87d3fd8cb38c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 47717\n",
       "})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://discuss.huggingface.co/t/the-model-did-not-return-a-loss-from-the-inputs-only-the-following-keys-logits-for-reference-the-inputs-it-received-are-input-values/25420/9\n",
    "train_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c45879-ad6f-4ca9-825e-c46516b0d31d",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bfc81c3-0821-4dc6-8ed6-8e7258dda4ab",
   "metadata": {},
   "source": [
    "## Load OPT-125M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a95c76b8-930c-48d3-a839-492740fb4162",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from optimum.bettertransformer import BetterTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cabdc7b7-a596-4552-a8b8-ec58af827fb4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "model.config.use_cache = False\n",
    "# model = BetterTransformer.transform(model, keep_original_model=True) # https://huggingface.co/docs/optimum/bettertransformer/tutorials/convert#training-compatibility"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a00d8da6-22b8-42b0-98e2-e6f469d6e1a7",
   "metadata": {},
   "source": [
    "## Fine-Tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "725742cf-c173-481d-963e-0b063e2b75b6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 7.5\n",
      "CUDA SETUP: Detected CUDA version 113\n",
      "CUDA SETUP: Loading binary /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda113.so...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:147: UserWarning: /opt/conda did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...\n",
      "  warn(msg)\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "397552ca-b683-46b9-a7d8-4686913808d8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmariafshan\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jupyter/Data/wandb/run-20231204_014949-lfqzqla0</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/mariafshan/MLOps_OPT_125_v2/runs/lfqzqla0' target=\"_blank\">royal-mountain-3</a></strong> to <a href='https://wandb.ai/mariafshan/MLOps_OPT_125_v2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/mariafshan/MLOps_OPT_125_v2' target=\"_blank\">https://wandb.ai/mariafshan/MLOps_OPT_125_v2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/mariafshan/MLOps_OPT_125_v2/runs/lfqzqla0' target=\"_blank\">https://wandb.ai/mariafshan/MLOps_OPT_125_v2/runs/lfqzqla0</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5727' max='5727' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5727/5727 2:40:20, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.392400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.776300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.491900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.334100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.217900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.167400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.135900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.106000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.076600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.067000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.058300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=5727, training_loss=0.3359923796523027, metrics={'train_runtime': 9622.5015, 'train_samples_per_second': 14.877, 'train_steps_per_second': 0.595, 'total_flos': 1.8702107836416e+16, 'train_loss': 0.3359923796523027, 'epoch': 3.0})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"model/result_v2/\",\n",
    "    push_to_hub=False,\n",
    "    evaluation_strategy = \"no\",\n",
    "    use_cpu = False,\n",
    "    per_device_train_batch_size = 25, # i want to speed up the training\n",
    "    learning_rate = 2e-4 \n",
    ")\n",
    "\n",
    "wandb.init(\n",
    "    # set the wandb project where this run will be logged\n",
    "    project=\"MLOps_OPT_125_v2\",\n",
    "    \n",
    "    # track hyperparameters and run metadata\n",
    "    config = training_args\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset = train_data,\n",
    "    eval_dataset = validation_data,\n",
    "    data_collator = data_collator\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0395e6b8-28cd-4dc5-9412-39b4e2bc87ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.030 MB of 0.030 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train/epoch</td><td>▁▂▂▃▄▄▅▆▆▇██</td></tr><tr><td>train/global_step</td><td>▁▂▂▃▄▄▅▆▆▇██</td></tr><tr><td>train/learning_rate</td><td>█▇▇▆▅▄▄▃▂▂▁</td></tr><tr><td>train/loss</td><td>█▅▃▂▂▂▁▁▁▁▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>train/epoch</td><td>3.0</td></tr><tr><td>train/global_step</td><td>5727</td></tr><tr><td>train/learning_rate</td><td>1e-05</td></tr><tr><td>train/loss</td><td>0.0583</td></tr><tr><td>train/total_flos</td><td>1.8702107836416e+16</td></tr><tr><td>train/train_loss</td><td>0.33599</td></tr><tr><td>train/train_runtime</td><td>9622.5015</td></tr><tr><td>train/train_samples_per_second</td><td>14.877</td></tr><tr><td>train/train_steps_per_second</td><td>0.595</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">royal-mountain-3</strong> at: <a href='https://wandb.ai/mariafshan/MLOps_OPT_125_v2/runs/lfqzqla0' target=\"_blank\">https://wandb.ai/mariafshan/MLOps_OPT_125_v2/runs/lfqzqla0</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231204_014949-lfqzqla0/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "new_model_name = \"model/opt_125_data_v2\"\n",
    "\n",
    "trainer.save_model(new_model_name)\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98cf0e95-ca13-4955-8257-b8c1ae1e2adf",
   "metadata": {},
   "source": [
    "# Test the New Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bda4c0a9-f587-4b71-a270-73b1452267ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model = AutoModelForCausalLM.from_pretrained(new_model_name)\n",
    "new_model.config.use_cache = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "646dc16e-d009-4584-b98b-a15c553c1980",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(text, model, tokenizer, max_input_tokens = 1000, max_output_tokens = 100):\n",
    "    device = model.device\n",
    "    # Tokenize\n",
    "    input_ids = tokenizer.encode(text, return_tensors=\"pt\", truncation=True, max_length=max_input_tokens).to(device)\n",
    "\n",
    "    # Generate\n",
    "    generated_tokens = model.generate(input_ids=input_ids.to(device), max_length=max_output_tokens, temperature = 0.4, pad_token_id=tokenizer.eos_token_id, do_sample = True)\n",
    "\n",
    "    # Decode\n",
    "    generated_text = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n",
    "    \n",
    "    # Strip the prompt\n",
    "    generated_text_answer = generated_text[0][len(text):]\n",
    "    \n",
    "    return generated_text_answer\n",
    "\n",
    "def qa_gen(text, model, tokenizer, max_output_tokens = 100):\n",
    "    # instruction = \"instruction: please answer the following question\\n\"\n",
    "    question = \"question: \" + str(text) + \"\\n\"\n",
    "    prompt = question + \"answer:\"\n",
    "    print(prompt)\n",
    "    print(\"-------------------BELOW IS GENERATED BY LANGUAGE MODEL---------------------------\")\n",
    "    print(inference(text = prompt, model = model, tokenizer = tokenizer, max_output_tokens = max_output_tokens))\n",
    "    print(\"-------------------END OF TEXT GENERATED BY LANGUAGE MODEL------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83ff131d-8408-4760-af17-15e883b5d04b",
   "metadata": {},
   "source": [
    "## Zero-Shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9d3801d3-0363-488b-9a64-e9a7434e07df",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 17 µs, sys: 0 ns, total: 17 µs\n",
      "Wall time: 21.7 µs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'What types of exercise are best for people with asthma?'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "test_prompt = [\"What types of exercise are best for people with asthma?\", \"How is obsessive-compulsive disorder diagnosed?\", \"When are you more likely to get a blood clot?\", \"How should you lift objects to prevent back pain?\", \"How can you be smart with antibiotics?\"]\n",
    "\n",
    "test_prompt[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1a7e70eb-beef-4ba0-8b8c-f98b1b000a19",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question: What types of exercise are best for people with asthma?\n",
      "answer:\n",
      "-------------------BELOW IS GENERATED BY LANGUAGE MODEL---------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1554: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Yes, those cheap calories give you some relief from inflammation. But when you have an asthma attack, it's important to know what you can do to prevent an attack so you can stop it before it starts. Here are some simple tips that can help you prevent an asthma attack. Call your doctor if you have any of these symptoms: Difficulty breathing or shortness of breath or wheezing Coughing up blood\n",
      "-------------------END OF TEXT GENERATED BY LANGUAGE MODEL------------------------\n",
      "\n",
      "question: How is obsessive-compulsive disorder diagnosed?\n",
      "answer:\n",
      "-------------------BELOW IS GENERATED BY LANGUAGE MODEL---------------------------\n",
      " A very strange or unusual explanation is given after a person has a seizure: probably a very gradual decline in behavior.\n",
      "\n",
      "    ### answer: A very strange or strange explanation is given during a seizure: probably a consequence of the medicine being used for someone who has a seizure.\n",
      "     ### answer: So after a seizure, you are likely to have some other problems, including: Difficulty swallowing and/\n",
      "-------------------END OF TEXT GENERATED BY LANGUAGE MODEL------------------------\n",
      "\n",
      "question: When are you more likely to get a blood clot?\n",
      "answer:\n",
      "-------------------BELOW IS GENERATED BY LANGUAGE MODEL---------------------------\n",
      " Yes, this is a common symptom of psoriasis. It's a skin condition that causes raised, reddish, and sometimes very red patches, too. Yes, you can get some complications, but most cases are minor and will go away on their own. Your doctor may need to do a special procedure to make sure your blood clot doesn't break free. Sometimes, though, it can be a sign of\n",
      "-------------------END OF TEXT GENERATED BY LANGUAGE MODEL------------------------\n",
      "\n",
      "question: How should you lift objects to prevent back pain?\n",
      "answer:\n",
      "-------------------BELOW IS GENERATED BY LANGUAGE MODEL---------------------------\n",
      " Osteoarthritis, which also causes degeneration of the hip joint, is an extremely common and bothersome disease. It is a disease that can be cured or controlled by wearing the right kind of protective gear. It can be one of the following things: 1. Even if you don't wear the same pair every day for weeks or months, it's still a good idea to see a doctor. Even if\n",
      "-------------------END OF TEXT GENERATED BY LANGUAGE MODEL------------------------\n",
      "\n",
      "question: How can you be smart with antibiotics?\n",
      "answer:\n",
      "-------------------BELOW IS GENERATED BY LANGUAGE MODEL---------------------------\n",
      " It depends on how bad it is.\n",
      "\n",
      "    ### Given the context information:\n",
      "    ### Given the context information, answer the question: How can you avoid getting an infection?\n",
      "   \n",
      "    ### answer: It depends on how bad it is. Get the facts, and learn why you can get the symptoms, and what you can do about it. An infection happens when your immune\n",
      "-------------------END OF TEXT GENERATED BY LANGUAGE MODEL------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for prompt in test_prompt:\n",
    "    qa_gen(text = prompt, model = model, tokenizer = tokenizer, max_output_tokens = 100)\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": ".m113",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/:m113"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
